{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ad8dc0",
   "metadata": {},
   "source": [
    "Roshan Gautam\n",
    "University of Cumberlands\n",
    "Data Mining \n",
    "Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcb357",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== ADVANCED DATA MINING LAB 6 ===\")\n",
    "print(\"Association Rule Mining with Apriori and FP-Growth\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "print(\"\\n1. DATA PREPARATION\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Load the Online Retail dataset\n",
    "try:\n",
    "    df = pd.read_excel('Online_Retail.xlsx')\n",
    "    print(f\"Dataset loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Online_Retail.xlsx not found. Please download from UCI ML Repository.\")\n",
    "    exit()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Data cleaning\n",
    "print(\"\\nData Cleaning:\")\n",
    "print(f\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with missing CustomerID and Description\n",
    "df_clean = df.dropna(subset=['CustomerID', 'Description'])\n",
    "\n",
    "# Remove cancelled transactions (negative quantities)\n",
    "df_clean = df_clean[df_clean['Quantity'] > 0]\n",
    "\n",
    "# Remove transactions with invalid unit prices\n",
    "df_clean = df_clean[df_clean['UnitPrice'] > 0]\n",
    "\n",
    "print(f\"\\nAfter cleaning: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "print(f\"Unique customers: {df_clean['CustomerID'].nunique()}\")\n",
    "print(f\"Unique products: {df_clean['Description'].nunique()}\")\n",
    "\n",
    "# Step 1 Visualizations: Dataset Exploration\n",
    "print(\"\\nCreating exploratory visualizations...\")\n",
    "\n",
    "# Most frequently purchased items\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_items = df_clean['Description'].value_counts().head(15)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=top_items.values, y=top_items.index)\n",
    "plt.title('Top 15 Most Frequently Purchased Items')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "# Transaction distribution by month\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "df_clean['Month'] = df_clean['InvoiceDate'].dt.month\n",
    "plt.subplot(1, 2, 2)\n",
    "monthly_transactions = df_clean.groupby('Month')['InvoiceNo'].nunique()\n",
    "sns.barplot(x=monthly_transactions.index, y=monthly_transactions.values)\n",
    "plt.title('Number of Transactions by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare transaction data for market basket analysis\n",
    "print(\"\\nPreparing transaction data...\")\n",
    "\n",
    "# Filter to top N most popular items to reduce memory usage\n",
    "top_n_items = 150\n",
    "print(f\"Filtering to top {top_n_items} most popular items for memory optimization...\")\n",
    "top_items = df_clean['Description'].value_counts().head(top_n_items).index.tolist()\n",
    "df_filtered = df_clean[df_clean['Description'].isin(top_items)]\n",
    "\n",
    "print(f\"Filtered dataset: {df_filtered.shape[0]} rows with {len(top_items)} unique products\")\n",
    "\n",
    "# Group by InvoiceNo to create transaction baskets\n",
    "transactions = df_filtered.groupby('InvoiceNo')['Description'].apply(list).tolist()\n",
    "\n",
    "# Filter transactions with at least 2 items\n",
    "transactions = [t for t in transactions if len(t) >= 2]\n",
    "print(f\"Number of transactions with 2+ items: {len(transactions)}\")\n",
    "\n",
    "# Convert to one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"One-hot encoded data shape: {df_encoded.shape}\")\n",
    "print(f\"Sample of encoded data:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Create item co-occurrence heatmap for top items\n",
    "print(\"\\nCreating item co-occurrence heatmap...\")\n",
    "top_20_items = df_filtered['Description'].value_counts().head(20).index\n",
    "df_top_items = df_encoded[top_20_items]\n",
    "\n",
    "# Calculate co-occurrence matrix\n",
    "cooccurrence_matrix = df_top_items.T.dot(df_top_items)\n",
    "np.fill_diagonal(cooccurrence_matrix.values, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cooccurrence_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[item[:30] + '...' if len(item) > 30 else item for item in top_20_items],\n",
    "            yticklabels=[item[:30] + '...' if len(item) > 30 else item for item in top_20_items])\n",
    "plt.title('Item Co-occurrence Matrix (Top 20 Items)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Frequent Itemset Mining Using Apriori\n",
    "print(\"\\n2. FREQUENT ITEMSET MINING USING APRIORI\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Set support threshold (higher due to filtered dataset)\n",
    "min_support = 0.02\n",
    "print(f\"Using minimum support threshold: {min_support}\")\n",
    "print(f\"This means itemsets must appear in at least {int(min_support * len(df_encoded))} transactions\")\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "print(\"Running Apriori algorithm...\")\n",
    "start_time = time.time()\n",
    "frequent_itemsets_apriori = apriori(df_encoded, min_support=min_support, use_colnames=True)\n",
    "apriori_time = time.time() - start_time\n",
    "\n",
    "print(f\"Apriori execution time: {apriori_time:.4f} seconds\")\n",
    "print(f\"Number of frequent itemsets found: {len(frequent_itemsets_apriori)}\")\n",
    "\n",
    "# Display top frequent itemsets\n",
    "frequent_itemsets_apriori['length'] = frequent_itemsets_apriori['itemsets'].apply(lambda x: len(x))\n",
    "print(f\"\\nFrequent itemsets by length:\")\n",
    "print(frequent_itemsets_apriori['length'].value_counts().sort_index())\n",
    "\n",
    "# Show top 10 frequent itemsets\n",
    "print(f\"\\nTop 10 frequent itemsets (by support):\")\n",
    "top_itemsets_apriori = frequent_itemsets_apriori.nlargest(10, 'support')\n",
    "for idx, row in top_itemsets_apriori.iterrows():\n",
    "    items = ', '.join(list(row['itemsets']))\n",
    "    print(f\"Support: {row['support']:.4f} | Items: {items}\")\n",
    "\n",
    "# Visualization: Top frequent itemsets\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_15_itemsets = frequent_itemsets_apriori.nlargest(15, 'support')\n",
    "itemset_labels = [', '.join(list(itemset)[:2]) + ('...' if len(itemset) > 2 else '') \n",
    "                  for itemset in top_15_itemsets['itemsets']]\n",
    "\n",
    "sns.barplot(data=top_15_itemsets.reset_index(), y=range(len(top_15_itemsets)), x='support')\n",
    "plt.yticks(range(len(top_15_itemsets)), itemset_labels)\n",
    "plt.title('Top 15 Frequent Itemsets (Apriori)')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Itemsets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Frequent Itemset Mining Using FP-Growth\n",
    "print(\"\\n3. FREQUENT ITEMSET MINING USING FP-GROWTH\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "# Apply FP-Growth algorithm with same support threshold\n",
    "print(\"Running FP-Growth algorithm...\")\n",
    "start_time = time.time()\n",
    "frequent_itemsets_fpgrowth = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "fpgrowth_time = time.time() - start_time\n",
    "\n",
    "print(f\"FP-Growth execution time: {fpgrowth_time:.4f} seconds\")\n",
    "print(f\"Number of frequent itemsets found: {len(frequent_itemsets_fpgrowth)}\")\n",
    "\n",
    "# Display top frequent itemsets\n",
    "frequent_itemsets_fpgrowth['length'] = frequent_itemsets_fpgrowth['itemsets'].apply(lambda x: len(x))\n",
    "print(f\"\\nFrequent itemsets by length:\")\n",
    "print(frequent_itemsets_fpgrowth['length'].value_counts().sort_index())\n",
    "\n",
    "# Show top 10 frequent itemsets\n",
    "print(f\"\\nTop 10 frequent itemsets (by support):\")\n",
    "top_itemsets_fpgrowth = frequent_itemsets_fpgrowth.nlargest(10, 'support')\n",
    "for idx, row in top_itemsets_fpgrowth.iterrows():\n",
    "    items = ', '.join(list(row['itemsets']))\n",
    "    print(f\"Support: {row['support']:.4f} | Items: {items}\")\n",
    "\n",
    "# Visualization: Top frequent itemsets from FP-Growth\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_15_itemsets_fp = frequent_itemsets_fpgrowth.nlargest(15, 'support')\n",
    "itemset_labels_fp = [', '.join(list(itemset)[:2]) + ('...' if len(itemset) > 2 else '') \n",
    "                     for itemset in top_15_itemsets_fp['itemsets']]\n",
    "\n",
    "sns.barplot(data=top_15_itemsets_fp.reset_index(), y=range(len(top_15_itemsets_fp)), x='support')\n",
    "plt.yticks(range(len(top_15_itemsets_fp)), itemset_labels_fp)\n",
    "plt.title('Top 15 Frequent Itemsets (FP-Growth)')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Itemsets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Generating and Analyzing Association Rules\n",
    "print(\"\\n4. GENERATING AND ANALYZING ASSOCIATION RULES\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "# Set confidence threshold\n",
    "min_confidence = 0.2\n",
    "print(f\"Using minimum confidence threshold: {min_confidence}\")\n",
    "\n",
    "# Generate association rules from Apriori results\n",
    "print(\"\\nGenerating association rules from Apriori results...\")\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", \n",
    "                                min_threshold=min_confidence)\n",
    "\n",
    "if len(rules_apriori) > 0:\n",
    "    print(f\"Number of association rules (Apriori): {len(rules_apriori)}\")\n",
    "    \n",
    "    # Display top rules by confidence\n",
    "    print(f\"\\nTop 10 association rules by confidence (Apriori):\")\n",
    "    top_rules_apriori = rules_apriori.nlargest(10, 'confidence')\n",
    "    for idx, row in top_rules_apriori.iterrows():\n",
    "        antecedents = ', '.join(list(row['antecedents']))\n",
    "        consequents = ', '.join(list(row['consequents']))\n",
    "        print(f\"Rule: {antecedents} → {consequents}\")\n",
    "        print(f\"  Support: {row['support']:.4f}, Confidence: {row['confidence']:.4f}, Lift: {row['lift']:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Generate association rules from FP-Growth results\n",
    "print(\"Generating association rules from FP-Growth results...\")\n",
    "rules_fpgrowth = association_rules(frequent_itemsets_fpgrowth, metric=\"confidence\", \n",
    "                                 min_threshold=min_confidence)\n",
    "\n",
    "if len(rules_fpgrowth) > 0:\n",
    "    print(f\"Number of association rules (FP-Growth): {len(rules_fpgrowth)}\")\n",
    "    \n",
    "    # Display top rules by lift\n",
    "    print(f\"\\nTop 10 association rules by lift (FP-Growth):\")\n",
    "    top_rules_fpgrowth = rules_fpgrowth.nlargest(10, 'lift')\n",
    "    for idx, row in top_rules_fpgrowth.iterrows():\n",
    "        antecedents = ', '.join(list(row['antecedents']))\n",
    "        consequents = ', '.join(list(row['consequents']))\n",
    "        print(f\"Rule: {antecedents} → {consequents}\")\n",
    "        print(f\"  Support: {row['support']:.4f}, Confidence: {row['confidence']:.4f}, Lift: {row['lift']:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Visualization: Confidence vs Lift scatter plot\n",
    "if len(rules_apriori) > 0:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(rules_apriori['confidence'], rules_apriori['lift'], alpha=0.6, s=rules_apriori['support']*1000)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.title('Association Rules: Confidence vs Lift (Apriori)\\nBubble size = Support')\n",
    "    plt.axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    if len(rules_fpgrowth) > 0:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(rules_fpgrowth['confidence'], rules_fpgrowth['lift'], alpha=0.6, s=rules_fpgrowth['support']*1000)\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Lift')\n",
    "        plt.title('Association Rules: Confidence vs Lift (FP-Growth)\\nBubble size = Support')\n",
    "        plt.axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 5: Comparative Analysis\n",
    "print(\"\\n5. COMPARATIVE ANALYSIS\")\n",
    "print(\"-\" * 24)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Apriori execution time: {apriori_time:.4f} seconds\")\n",
    "print(f\"FP-Growth execution time: {fpgrowth_time:.4f} seconds\")\n",
    "print(f\"Speed improvement: {apriori_time/fpgrowth_time:.2f}x faster with FP-Growth\")\n",
    "\n",
    "print(f\"\\nResults Comparison:\")\n",
    "print(f\"Frequent itemsets found:\")\n",
    "print(f\"  Apriori: {len(frequent_itemsets_apriori)}\")\n",
    "print(f\"  FP-Growth: {len(frequent_itemsets_fpgrowth)}\")\n",
    "\n",
    "if len(rules_apriori) > 0 and len(rules_fpgrowth) > 0:\n",
    "    print(f\"Association rules generated:\")\n",
    "    print(f\"  Apriori: {len(rules_apriori)}\")\n",
    "    print(f\"  FP-Growth: {len(rules_fpgrowth)}\")\n",
    "\n",
    "# Algorithm efficiency visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Execution time comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "algorithms = ['Apriori', 'FP-Growth']\n",
    "times = [apriori_time, fpgrowth_time]\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "bars = plt.bar(algorithms, times, color=colors)\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "for bar, time in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{time:.4f}s', ha='center', va='bottom')\n",
    "\n",
    "# Itemsets found comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "itemset_counts = [len(frequent_itemsets_apriori), len(frequent_itemsets_fpgrowth)]\n",
    "bars = plt.bar(algorithms, itemset_counts, color=colors)\n",
    "plt.ylabel('Number of Frequent Itemsets')\n",
    "plt.title('Frequent Itemsets Found')\n",
    "for bar, count in zip(bars, itemset_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LAB COMPLETION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(\"Data preparation and cleaning completed\")\n",
    "print(\"Exploratory data analysis with visualizations\")\n",
    "print(\"Apriori algorithm implementation\")\n",
    "print(\"FP-Growth algorithm implementation\")\n",
    "print(\"Association rule generation and analysis\")\n",
    "print(\"Comparative analysis between algorithms\")\n",
    "print(\"Performance and efficiency evaluation\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
